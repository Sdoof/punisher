{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from punisher.common import *\n",
    "\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_AB(y1, y2, x=None, fs=(20,12), title=None):\n",
    "    if x is None:\n",
    "        x = np.array([i for i in range(len(y1))])\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(fs)\n",
    "    plt.title(title)\n",
    "    plt.plot(x, y1, label='preds')\n",
    "    plt.plot(x, y2, label='targs')\n",
    "    plt.grid()\n",
    "    legend = ax.legend(loc='upper left')\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def get_price_data(asset, exchange_id, timeframe, start, end, lead=750, lag=3000):\n",
    "    exchange = load_exchange(exchange_id)\n",
    "    fpath = ohlcv_feed.get_ohlcv_fpath(asset, exchange_id, timeframe)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(fpath)\n",
    "        ohlcv_feed.fetch_and_save_asset(exchange, asset, timeframe, start, end)\n",
    "    df = ohlcv_feed.load_asset(fpath)\n",
    "    df.sort_values(by='utc', inplace=True)\n",
    "    close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "    volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "    df['lead'] = df[close_col].rolling(lead).mean()\n",
    "    df['lag'] = df[close_col].rolling(lag).mean()\n",
    "    #df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def plot_price(df, field, asset, ex_id):\n",
    "    col_name = ohlcv_feed.get_col_name(field, asset.symbol, ex_id)\n",
    "    punisher.utils.charts.plot_range(\n",
    "        df, start=None, end=None, \n",
    "        column_name=col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coinmarketcap.com/api/\n",
    "exchange_id = ex_cfg.POLONIEX\n",
    "asset = Asset(coins.ETH, coins.BTC)\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "end = datetime.datetime(year=2018, month=1, day=1)\n",
    "timeframe = Timeframe.ONE_DAY\n",
    "exchange = load_exchange(exchange_id)\n",
    "df = get_price_data(asset, exchange_id, timeframe, start, end)\n",
    "close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['utc'] < datetime.datetime(year=2017, month=9, day=1)]\n",
    "val = df[df['utc'] >= datetime.datetime(year=2017, month=9, day=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_price(train, 'close', asset, exchange_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://medium.com/@eliquinox/cryptocurrency-data-analysis-part-iii-backtesting-evaluating-and-optimising-a-trading-strategy-9bc9b1179a8b\n",
    "* https://www.investopedia.com/university/movingaverage/movingaverages4.asp\n",
    "* https://blog.patricktriest.com/analyzing-cryptocurrencies-python/\n",
    "* https://github.com/AdamStone/cryptrade\n",
    "* https://pythonprogramming.net/advanced-matplotlib-graphing-charting-tutorial/\n",
    "* https://www.tradingview.com/script/TuG4VjJX-Crypto-Adjusted-Moving-Average-CAMA/\n",
    "* https://www.tradingview.com/cryptocurrency-signals/\n",
    "* https://romanorac.github.io/cryptocurrency/analysis/2017/12/29/cryptocurrency-analysis-with-python-part3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sma_df = df.copy()\n",
    "sma_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sma_df['lead'] = sma_df[close_col].rolling(250).mean()\n",
    "sma_df['lag'] = sma_df[close_col].rolling(500).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sma_df[[close_col,'lead','lag']].plot(figsize = (16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lead, lag = 1000, 3000\n",
    "pc_thresh = .025\n",
    "\n",
    "ma_df = sma_df.copy()\n",
    "ma_df['lead'] = ma_df[close_col].rolling(lead).mean()\n",
    "ma_df['lag'] = ma_df[close_col].rolling(lag).mean()\n",
    "ma_df.dropna(inplace = True)\n",
    "ma_df['lead-lag'] = ma_df['lead'] - ma_df['lag']\n",
    "ma_df['pc_diff'] = ma_df['lead-lag'] / ma_df[close_col]\n",
    "ma_df['regime'] = np.where(ma_df['pc_diff'] > pc_thresh, 1, 0)\n",
    "ma_df['regime'] = np.where(ma_df['pc_diff'] < -pc_thresh, -1, ma_df['regime'])\n",
    "ma_df['Market'] = np.log(ma_df[close_col] / ma_df[close_col].shift(1))\n",
    "ma_df['Strategy'] = ma_df['regime'].shift(1) * ma_df['Market']\n",
    "ma_df[['Market','Strategy']] = ma_df[['Market','Strategy']].cumsum().apply(np.exp)\n",
    "ma_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ma_df['regime'].plot(figsize=(16,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ma_df[['Market','Strategy']].iloc[-1]\n",
    "ma_df[['Market','Strategy']].plot(figsize = (16,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MACD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://romanorac.github.io/cryptocurrency/analysis/2017/12/17/cryptocurrency-analysis-with-python-part1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numpy Close Data Array\n",
    "all_utc_arr = np.array(df['utc'])\n",
    "print(all_utc_arr.shape)\n",
    "\n",
    "trn_arr = np.array(train[close_col])\n",
    "trn_utc_arr = np.array(train['utc'])\n",
    "\n",
    "val_arr = np.array(val[close_col])\n",
    "val_utc_arr = np.array(val['utc'])\n",
    "print(trn_arr.shape, trn_utc_arr.shape,val_arr.shape,val_utc_arr.shape)\n",
    "\n",
    "plt.plot(trn_utc_arr, trn_arr)\n",
    "plt.plot(val_utc_arr, val_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inp_targs(arr, prior_periods, target_period):\n",
    "    # Categorical\n",
    "    # 30 minute timeframe\n",
    "    # 2 years of data = 35088\n",
    "    # 48 periods / day\n",
    "    # Input = 24 periods, output = lower/neutral/higher \n",
    "    # (after 12 periods)\n",
    "    inp = []\n",
    "    targs = []\n",
    "    for i in range(0, len(arr[:-target_period])):\n",
    "        start_close = arr[i]\n",
    "        end_close = arr[i+target_period]\n",
    "        pct_delta = (end_close - start_close) / start_close\n",
    "        inp.append(arr[i:i+prior_periods])\n",
    "        targs.append(pct_delta)\n",
    "    inp = np.array(inp)\n",
    "    targs = np.expand_dims(np.array(targs), axis=1)\n",
    "    print(inp.shape, targs.shape)\n",
    "    return inp,targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_inp,trn_targs = get_inp_targs(trn_arr, 12, 24)\n",
    "val_inp,val_targs = get_inp_targs(val_arr, 12, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression(normalize=True)\n",
    "\n",
    "linreg.fit(trn_inp, trn_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept and Coefficients\n",
    "print (linreg.intercept_)\n",
    "print (linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "trn_preds = linreg.predict(trn_inp)\n",
    "val_preds = linreg.predict(val_inp)\n",
    "trn_preds.shape, trn_targs.shape, trn_utc_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_utc_arr = trn_utc_arr[:len(trn_targs)]\n",
    "plot_AB(trn_preds, trn_targs, x=np.expand_dims(trn_utc_arr,1), \n",
    "        title='Linear Regress Train Preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_utc_arr = val_utc_arr[:len(val_targs)]\n",
    "plot_AB(val_preds, val_targs, x=np.expand_dims(val_utc_arr,1), title='Linear Regress Val Preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/\n",
    "print(\"MAE\", metrics.mean_absolute_error(trn_targs, trn_preds))\n",
    "print(\"MSE\", metrics.mean_squared_error(trn_targs, trn_preds))\n",
    "print(\"RMSE\", np.sqrt(metrics.mean_squared_error(trn_targs, trn_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/\n",
    "print(\"MAE\", metrics.mean_absolute_error(val_targs, val_preds))\n",
    "print(\"MSE\", metrics.mean_squared_error(val_targs, val_preds))\n",
    "print(\"RMSE\", np.sqrt(metrics.mean_squared_error(val_targs, val_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Close)\n",
    "\n",
    "* https://github.com/bfortuner/ml-study/blob/master/LogisticRegression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(pct_targs, threshold):\n",
    "    targs = []\n",
    "    for targ in pct_targs:\n",
    "        targ = targ[0]\n",
    "        if targ < -threshold:\n",
    "            targs.append(0)\n",
    "        elif targ > threshold:\n",
    "            targs.append(2)\n",
    "        else:\n",
    "            targs.append(1)\n",
    "    return np.expand_dims(np.array(targs),1)\n",
    "\n",
    "def get_one_hot_categorical(targs):\n",
    "    onehots = preprocessing.OneHotEncoder()\n",
    "    return onehots.fit(targs).transform(targs).toarray()\n",
    "\n",
    "# trn_onehots = get_one_hot_categorical(trn_targs, .01)\n",
    "# val_onehots = get_one_hot_categorical(trn_targs, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Close Data Array\n",
    "all_utc_arr = np.array(df['utc'])\n",
    "print(all_utc_arr.shape)\n",
    "\n",
    "trn_arr = np.array(train[close_col])\n",
    "trn_utc_arr = np.array(train['utc'])\n",
    "\n",
    "val_arr = np.array(val[close_col])\n",
    "val_utc_arr = np.array(val['utc'])\n",
    "print(trn_arr.shape, trn_utc_arr.shape,val_arr.shape,val_utc_arr.shape)\n",
    "\n",
    "plt.plot(trn_utc_arr, trn_arr)\n",
    "plt.plot(val_utc_arr, val_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .02\n",
    "trn_inp,trn_targs = get_inp_targs(trn_arr, 12, 24)\n",
    "val_inp,val_targs = get_inp_targs(val_arr, 12, 24)\n",
    "trn_labels = get_labels(trn_targs, threshold).ravel()   #Logistic regression expects (n,) shape\n",
    "val_labels = get_labels(val_targs, threshold).ravel()\n",
    "normalized_range = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "trn_inp = normalized_range.fit_transform(trn_inp)\n",
    "val_inp = normalized_range.fit_transform(val_inp)\n",
    "\n",
    "trn_labels.shape,val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(trn_inp, trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels\n",
    "trn_preds = model.predict(trn_inp)\n",
    "val_preds = model.predict(val_inp)\n",
    "trn_preds,val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "trn_probs = model.predict_proba(trn_inp)\n",
    "val_probs = model.predict_proba(val_inp)\n",
    "trn_probs.shape,val_probs.shape\n",
    "\n",
    "preds = np.argmax(trn_probs,axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Trn Acc\", model.score(trn_inp, trn_labels))\n",
    "print(\"Val Acc\", model.score(val_inp, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Loss\n",
    "print(\"Trn Cross Entropy\", metrics.log_loss(trn_labels, trn_probs))\n",
    "print(\"Val Cross Entropy\", metrics.log_loss(val_labels, val_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(trn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets\n",
    "plt.hist(trn_preds, bins=3, range=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trn_labels, bins=3, alpha=0.5, label='label')\n",
    "plt.hist(trn_preds, bins=3, alpha=0.5, label='pred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_labels, bins=3, alpha=0.5, label='label')\n",
    "plt.hist(val_preds, bins=3, alpha=0.5, label='pred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print (metrics.accuracy_score(trn_labels, trn_preds))\n",
    "print (metrics.accuracy_score(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print (metrics.classification_report(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# https://github.com/bfortuner/ml-study/blob/master/tools/ConfusionMatrix.ipynb \n",
    "print (metrics.confusion_matrix(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(val_labels, val_preds)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix, classes=['down','neutral','positive'],\n",
    "    title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Close + Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inp_targs(arr, prior_periods, target_period):\n",
    "    # Categorical\n",
    "    # 30 minute timeframe\n",
    "    # 2 years of data = 35088\n",
    "    # 48 periods / day\n",
    "    # Input = 24 periods, output = lower/neutral/higher \n",
    "    # (after 12 periods)\n",
    "    inp = []\n",
    "    targs = []\n",
    "    for i in range(0, len(arr[:-target_period,:])):\n",
    "        start_close = arr[i][0]\n",
    "        end_close = arr[i+target_period][0]\n",
    "        pct_delta = (end_close - start_close) / start_close\n",
    "        inp.append(arr[i:i+prior_periods])\n",
    "        targs.append(pct_delta)\n",
    "    inp = np.array(inp)\n",
    "    n_samples, timesteps, cols = inp.shape\n",
    "    inp = inp.reshape((n_samples, timesteps * cols))\n",
    "    targs = np.expand_dims(np.array(targs), axis=1)\n",
    "    print(inp.shape, targs.shape)\n",
    "    return inp,targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date =  datetime.datetime(year=2017, month=9, day=1)\n",
    "train = df[df['utc'] < split_date]\n",
    "val = df[df['utc'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Close Data Array\n",
    "all_utc_arr = np.array(df['utc'])\n",
    "print(all_utc_arr.shape)\n",
    "\n",
    "trn_arr = np.array(train[[close_col, volume_col]])\n",
    "trn_utc_arr = np.array(train['utc'])\n",
    "\n",
    "val_arr = np.array(val[[close_col, volume_col]])\n",
    "val_utc_arr = np.array(val['utc'])\n",
    "print(trn_arr.shape, trn_utc_arr.shape,val_arr.shape,val_utc_arr.shape)\n",
    "\n",
    "# Price\n",
    "plt.plot(trn_utc_arr, trn_arr[:,0])\n",
    "plt.plot(val_utc_arr, val_arr[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume\n",
    "plt.plot(trn_utc_arr, trn_arr[:,1])\n",
    "plt.plot(val_utc_arr, val_arr[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .02\n",
    "trn_inp,trn_targs = get_inp_targs(trn_arr, 12, 24)\n",
    "val_inp,val_targs = get_inp_targs(val_arr, 12, 24)\n",
    "trn_labels = get_labels(trn_targs, threshold).ravel()   #Logistic regression expects (n,) shape\n",
    "val_labels = get_labels(val_targs, threshold).ravel()\n",
    "normalized_range = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "trn_inp = normalized_range.fit_transform(trn_inp)\n",
    "val_inp = normalized_range.fit_transform(val_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(trn_inp, trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels\n",
    "trn_preds = model.predict(trn_inp)\n",
    "val_preds = model.predict(val_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "trn_probs = model.predict_proba(trn_inp)\n",
    "val_probs = model.predict_proba(val_inp)\n",
    "trn_probs.shape,val_probs.shape\n",
    "\n",
    "trn_preds,trn_probs,val_preds,val_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trn_labels, bins=3, alpha=0.5, label='label')\n",
    "plt.hist(trn_preds, bins=3, alpha=0.5, label='pred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_labels, bins=3, alpha=0.5, label='label')\n",
    "plt.hist(val_preds, bins=3, alpha=0.5, label='pred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print (\"Trn Acc\", metrics.accuracy_score(trn_labels, trn_preds))\n",
    "print (\"Val Acc\", metrics.accuracy_score(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(trn_labels, trn_preds)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix, classes=['down','neutral','positive'],\n",
    "    title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(val_labels, val_preds)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix, classes=['down','neutral','positive'],\n",
    "    title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inp_targs(arr, prior_periods, target_period):\n",
    "    # Categorical\n",
    "    # 30 minute timeframe\n",
    "    # 2 years of data = 35088\n",
    "    # 48 periods / day\n",
    "    # Input = 24 periods, output = lower/neutral/higher \n",
    "    # (after 12 periods)\n",
    "    inp = []\n",
    "    targs = []\n",
    "    for i in range(0, len(arr[:-target_period,:])):\n",
    "        start_close = arr[i][0]\n",
    "        end_close = arr[i+target_period][0]\n",
    "        pct_delta = (end_close - start_close) / start_close\n",
    "        inp.append(arr[i:i+prior_periods])\n",
    "        targs.append(pct_delta)\n",
    "    inp = np.array(inp)\n",
    "    n_samples, timesteps, cols = inp.shape\n",
    "    inp = inp.reshape((n_samples, timesteps * cols))\n",
    "    targs = np.expand_dims(np.array(targs), axis=1)\n",
    "    return inp,targs\n",
    "\n",
    "def get_log_reg_inputs(df, columns, prior_periods, target_period):\n",
    "    arr = np.array(df[columns])\n",
    "    threshold = .02\n",
    "    inp,targs = get_inp_targs(arr, prior_periods, target_period)\n",
    "    labels = get_labels(targs, threshold).ravel()\n",
    "    normalized_range = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "    inp = normalized_range.fit_transform(inp)\n",
    "    return inp,targs,labels\n",
    "\n",
    "def train_model(trn_inp, trn_labels):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(trn_inp, trn_labels)\n",
    "    return model\n",
    "\n",
    "def predict(model, inp):\n",
    "    # Predict class labels\n",
    "    preds = model.predict(inp)\n",
    "    probs = model.predict_proba(inp)\n",
    "    return preds,probs\n",
    "\n",
    "def evaluate(preds, probs, labels, plot_charts=True):\n",
    "    if plot_charts:\n",
    "        cnf_matrix = metrics.confusion_matrix(labels, preds)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(\n",
    "            cnf_matrix, classes=['down','neutral','positive'],\n",
    "            title='Confusion matrix')\n",
    "        plt.show()\n",
    "        plt.hist(labels, bins=3, alpha=0.5, label='label')\n",
    "        plt.hist(preds, bins=3, alpha=0.5, label='pred')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "    acc = metrics.accuracy_score(labels, preds)\n",
    "    logloss = metrics.log_loss(labels, probs)\n",
    "    return acc, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(columns, prior_periods, target_period):\n",
    "    trn_inp,trn_targs,trn_labels = get_log_reg_inputs(\n",
    "        train, columns, prior_periods, target_period)\n",
    "    val_inp,val_targs,val_labels = get_log_reg_inputs(\n",
    "        val, columns, prior_periods, target_period)\n",
    "\n",
    "    model = train_model(trn_inp, trn_labels)\n",
    "\n",
    "    trn_preds,trn_probs = predict(model, trn_inp)\n",
    "    val_preds,val_probs = predict(model, val_inp)\n",
    "\n",
    "    acc, loss = evaluate(trn_preds, trn_probs, trn_labels, False)\n",
    "    print(\"Trn - Acc: {:.4f} Loss: {:.4f}\".format(float(acc), float(loss)))\n",
    "    acc, loss = evaluate(val_preds, val_probs, val_labels, False)\n",
    "    print(\"val - Acc: {:.4f} Loss: {:.4f}\".format(float(acc), float(loss)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep inputs\n",
    "# https://coinmarketcap.com/api/\n",
    "exchange_id = ex_cfg.POLONIEX\n",
    "asset = Asset(coins.ETH, coins.BTC)\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "end = datetime.datetime(year=2018, month=1, day=1)\n",
    "timeframe = Timeframe.ONE_DAY\n",
    "exchange = load_exchange(exchange_id)\n",
    "df = get_price_data(asset, exchange_id, timeframe, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_period = 24\n",
    "prior_periods = 12\n",
    "\n",
    "close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "columns = [close_col, volume_col]\n",
    "\n",
    "split_date =  datetime.datetime(year=2017, month=9, day=1)\n",
    "train = df[df['utc'] < split_date]\n",
    "val = df[df['utc'] >= split_date]\n",
    "trn_inp,trn_targs,trn_labels = get_log_reg_inputs(\n",
    "    train, columns, prior_periods, target_period)\n",
    "val_inp,val_targs,val_labels = get_log_reg_inputs(\n",
    "    val, columns, prior_periods, target_period)\n",
    "val_inp.shape,val_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_periods = 12\n",
    "target_period = 24\n",
    "columns = [close_col, volume_col]\n",
    "run_experiment(columns, prior_periods, target_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# close seems best\n",
    "\n",
    "# 8 prior, 1 - 4 predict\n",
    "columns = [close_col]#, volume_col]\n",
    "for i in range(1,len(columns)+1):\n",
    "    cols = columns[:i]\n",
    "    for pp in [4,8,12,16,20,24]:\n",
    "        for tp in [pp+1, pp+2, pp+4, pp+6, pp+10, pp+12]:\n",
    "            print(pp, tp)\n",
    "            run_experiment(cols, pp, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Best Results (all 'close')\n",
    "\n",
    "prior_periods = 4\n",
    "target_period = 5 (next timestep)\n",
    "Trn - Acc: 0.8065 Loss: 0.5897\n",
    "val - Acc: 0.8188 Loss: 0.5779\n",
    "\n",
    "4 6\n",
    "Trn - Acc: 0.7819 Loss: 0.6430\n",
    "val - Acc: 0.7938 Loss: 0.6383\n",
    "\n",
    "4 8 \n",
    "Trn - Acc: 0.7344 Loss: 0.7347\n",
    "val - Acc: 0.7527 Loss: 0.7286\n",
    "\n",
    "-------\n",
    "\n",
    "8 9 \n",
    "Trn - Acc: 0.7240 Loss: 0.6964\n",
    "val - Acc: 0.7754 Loss: 0.6046\n",
    "\n",
    "8 10\n",
    "Trn - Acc: 0.7054 Loss: 0.7337\n",
    "val - Acc: 0.7552 Loss: 0.6635\n",
    "\n",
    "8 12 \n",
    "Trn - Acc: 0.6745 Loss: 0.7906\n",
    "val - Acc: 0.7310 Loss: 0.7294\n",
    "\n",
    "-----\n",
    "\n",
    "12 13\n",
    "Trn - Acc: 0.6853 Loss: 0.7308\n",
    "val - Acc: 0.7835 Loss: 0.6291\n",
    "\n",
    "12 14\n",
    "Trn - Acc: 0.6716 Loss: 0.7557\n",
    "val - Acc: 0.7646 Loss: 0.6627\n",
    "\n",
    "12 16\n",
    "Trn - Acc: 0.6466 Loss: 0.8015\n",
    "val - Acc: 0.7300 Loss: 0.7286\n",
    "\n",
    "\n",
    "Questions:\n",
    "1) When it's wrong, what are the financial consequences?\n",
    "2) When it's right, how much do we make?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://machinelearningmastery.com/time-series-forecasting-supervised-learning/\n",
    "* https://www.quantstart.com/articles/Forecasting-Financial-Time-Series-Part-1\n",
    "* http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep inputs\n",
    "# https://coinmarketcap.com/api/\n",
    "exchange_id = ex_cfg.POLONIEX\n",
    "asset = Asset(coins.ETH, coins.BTC)\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "end = datetime.datetime(year=2018, month=1, day=1)\n",
    "timeframe = Timeframe.THIRTY_MIN\n",
    "exchange = load_exchange(exchange_id)\n",
    "#ohlcv_feed.fetch_and_save_asset(exchange, asset, timeframe, start, end)\n",
    "#feed = OHLCVFileFeed([exchange_id], [asset], timeframe, start, end)\n",
    "fpath = ohlcv_feed.get_ohlcv_fpath(asset, exchange_id, timeframe)\n",
    "df = ohlcv_feed.load_asset(fpath)\n",
    "df.sort_values(by='utc', inplace=True)\n",
    "close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "df['lead'] = df[close_col].rolling(1000).mean()\n",
    "df['lag'] = df[close_col].rolling(5000).mean()\n",
    "df.dropna(inplace=True)\n",
    "df.head()\n",
    "\n",
    "split_date =  datetime.datetime(year=2017, month=9, day=1)\n",
    "train = df[df['utc'] < split_date]\n",
    "val = df[df['utc'] >= split_date]\n",
    "trn_inp,trn_targs,trn_labels = get_log_reg_inputs(\n",
    "    train, columns, prior_periods, target_period)\n",
    "val_inp,val_targs,val_labels = get_log_reg_inputs(\n",
    "    val, columns, prior_periods, target_period)\n",
    "print(\"Val\", val_inp.shape,val_targs.shape,val_labels.shape)\n",
    "unique_counts = np.unique(trn_labels, return_counts=True)\n",
    "unique_counts[0]\n",
    "plt.bar(left=unique_counts[0], height=unique_counts[1], tick_label=['negative','neutral', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm_experiment(columns, prior_periods, target_period):\n",
    "    trn_inp,trn_targs,trn_labels = get_log_reg_inputs(\n",
    "        train, columns, prior_periods, target_period)\n",
    "    val_inp,val_targs,val_labels = get_log_reg_inputs(\n",
    "        val, columns, prior_periods, target_period)\n",
    "\n",
    "    model = svm.SVC(probability=True)\n",
    "    model.fit(trn_inp, trn_labels)\n",
    "    \n",
    "    trn_preds,trn_probs = predict(model, trn_inp)\n",
    "    val_preds,val_probs = predict(model, val_inp)\n",
    "\n",
    "    acc, loss = evaluate(trn_preds, trn_probs, trn_labels, False)\n",
    "    print(\"Trn - Acc: {:.4f} Loss: {:.4f}\".format(float(acc), float(loss)))\n",
    "    acc, loss = evaluate(val_preds, val_probs, val_labels, False)\n",
    "    print(\"val - Acc: {:.4f} Loss: {:.4f}\".format(float(acc), float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [close_col]\n",
    "run_svm_experiment(columns, prior_periods, target_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [close_col, volume_col]\n",
    "run_svm_experiment(columns, prior_periods, target_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 8 prior, 1 - 4 predict\n",
    "columns = [close_col]#, volume_col, 'lead', 'lag']\n",
    "for i in range(1,len(columns)+1):\n",
    "    cols = columns[:i]\n",
    "    for pp in [4,8,12]#,16,20,24]:\n",
    "        for tp in [pp+1, pp+2, pp+4]#, pp+6, pp+10, pp+12]:\n",
    "            print(pp, tp, cols)\n",
    "            run_svm_experiment(cols, pp, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep inputs\n",
    "# https://coinmarketcap.com/api/\n",
    "exchange_id = ex_cfg.POLONIEX\n",
    "asset = Asset(coins.ETH, coins.BTC)\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "end = datetime.datetime(year=2018, month=1, day=1)\n",
    "timeframe = Timeframe.THIRTY_MIN\n",
    "exchange = load_exchange(exchange_id)\n",
    "df = get_price_data(asset, exchange_id, timeframe, start, end)\n",
    "target_period = 24\n",
    "prior_periods = 12\n",
    "close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "columns = [close_col, volume_col, 'lead', 'lag']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pct_change'] = df[[close_col]].pct_change()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pct_change', close_col]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inp_targs(arr, prior_periods, target_period):\n",
    "    inp = []\n",
    "    targs = []\n",
    "    for i in range(0, len(arr[:-target_period,:])):\n",
    "        start_close = arr[i][0]\n",
    "        end_close = arr[i+target_period][0]\n",
    "        pct_delta = (end_close - start_close) / start_close\n",
    "        inp.append(arr[i:i+prior_periods])\n",
    "        targs.append(pct_delta)\n",
    "    inp = np.array(inp)[:,:,1]\n",
    "    print(inp.shape)\n",
    "    n_samples, timesteps = inp.shape\n",
    "    inp = inp.reshape((n_samples, timesteps))\n",
    "    targs = np.expand_dims(np.array(targs), axis=1)\n",
    "    return inp,targs\n",
    "\n",
    "def get_log_reg_inputs(df, columns, prior_periods, target_period):\n",
    "    arr = np.array(df[columns])\n",
    "    threshold = .02\n",
    "    inp,targs = get_inp_targs(arr, prior_periods, target_period)\n",
    "    labels = get_labels(targs, threshold).ravel()\n",
    "    normalized_range = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "    inp = normalized_range.fit_transform(inp)\n",
    "    return inp,targs,labels\n",
    "\n",
    "def run_experiment(columns, prior_periods, target_period):\n",
    "    trn_inp,trn_targs,trn_labels = get_log_reg_inputs(\n",
    "        train, columns, prior_periods, target_period)\n",
    "    val_inp,val_targs,val_labels = get_log_reg_inputs(\n",
    "        val, columns, prior_periods, target_period)\n",
    "\n",
    "    model = train_model(trn_inp, trn_labels)\n",
    "\n",
    "    trn_preds,trn_probs = predict(model, trn_inp)\n",
    "    val_preds,val_probs = predict(model, val_inp)\n",
    "\n",
    "    acc, loss = evaluate(trn_preds, trn_probs, trn_labels, False)\n",
    "    print(\"Trn - Acc: {:.4f} Loss: {:.4f}\".format(float(acc), float(loss)))\n",
    "    acc, loss = evaluate(val_preds, val_probs, val_labels, False)\n",
    "    print(\"val - Acc: {:.4f} Loss: {:.4f}\".format(float(acc), float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [close_col, 'pct_change']\n",
    "split_date =  datetime.datetime(year=2017, month=9, day=1)\n",
    "train = df[df['utc'] < split_date]\n",
    "val = df[df['utc'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [close_col, 'pct_change']\n",
    "for pp in [4,8,12,16,20,24]:\n",
    "    for tp in [pp+1, pp+2, pp+4, pp+6, pp+10, pp+12]:\n",
    "        print(pp, tp, cols)\n",
    "        run_experiment(cols, pp, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://arxiv.org/pdf/1605.00003.pdf\n",
    "* https://medium.com/making-sense-of-data/time-series-next-value-prediction-using-regression-over-a-rolling-window-228f0acae363\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "* Auto Regressive Integrated Moving Average\n",
    "* https://dashee87.github.io/data%20science/general/A-Road-Incident-Model-Analysis/\n",
    "* https://machinelearningmastery.com/make-sample-forecasts-arima-python/\n",
    "* https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n",
    "* https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method\n",
    "* https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3\n",
    "* https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very good tutorial here:\n",
    "# https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3\n",
    "    \n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coinmarketcap.com/api/\n",
    "exchange_id = ex_cfg.POLONIEX\n",
    "asset = Asset(coins.ETH, coins.BTC)\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "end = datetime.datetime(year=2018, month=1, day=1)\n",
    "timeframe = Timeframe.ONE_DAY\n",
    "exchange = load_exchange(exchange_id)\n",
    "df = get_price_data(asset, exchange_id, timeframe, start, end)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_period = 24\n",
    "prior_periods = 12\n",
    "\n",
    "close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "columns = [close_col]\n",
    "\n",
    "close_utc = df[[close_col, 'utc']]\n",
    "plot_price(close_utc, 'close', asset, exchange_id)\n",
    "\n",
    "split_date =  datetime.datetime(year=2017, month=9, day=1)\n",
    "train = close_utc[close_utc['utc'] < split_date]\n",
    "val = close_utc[close_utc['utc'] >= split_date]\n",
    "train.set_index('utc', inplace=True)\n",
    "val.set_index('utc', inplace=True)\n",
    "close_utc.set_index('utc', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How correlated is price with last t time periods?\n",
    "\n",
    "close_utc = df[[close_col, 'utc']]\n",
    "close_utc.set_index('utc', inplace=True)\n",
    "close_utc.head()\n",
    "autocorrelation_plot(close_utc[:50])\n",
    "plt.show()\n",
    "autocorrelation_plot(close_utc[:100])\n",
    "plt.show()\n",
    "autocorrelation_plot(close_utc[:1000], )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* p = 5\n",
    "    * is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days.\n",
    "\n",
    "* d = 1\n",
    "    * is the integrated part of the model. This includes terms in the model that incorporate the amount of differencing (i.e. the number of past time points to subtract from the current value) to apply to the time series. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.\n",
    "* q = 0\n",
    "    * is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coef column shows the weight (i.e. importance) of each feature and how each one impacts\n",
    "# the time series. \n",
    "# The P>|z| column informs us of the significance of each feature weight. \n",
    "# Each weight has a p-value lower or close to 0.05, so it is reasonable to retain all \n",
    "# of them?\n",
    "\n",
    "model = ARIMA(train, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual errors\n",
    "# Our primary concern is to ensure that the residuals of our model are uncorrelated and \n",
    "# normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these \n",
    "# properties, it is a good indication that it can be further improved.\n",
    "\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "\n",
    "# Density plot of the residual error values, \n",
    "# Suggesting the errors are Gaussian, but may not be centered on zero.\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = close_utc.values\n",
    "size = int(len(X) * 0.95)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = metrics.mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "# plot\n",
    "plt.figure(figsize=(18,10))\n",
    "start = 0\n",
    "end = 100\n",
    "plt.plot(test[start:end], color='blue')\n",
    "plt.plot(predictions[start:end], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Prophet\n",
    "\n",
    "* https://github.com/facebook/prophet\n",
    "* https://msperlin.github.io/2017-03-05-Prophet-and_stock-market/\n",
    "* https://facebook.github.io/prophet/\n",
    "* https://github.com/facebook/prophet/blob/master/notebooks/quick_start.ipynb\n",
    "* https://github.com/facebook/prophet/blob/master/notebooks/non-daily_data.ipynb\n",
    "\n",
    "* You'll need to ```pip install rpy2```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext rpy2.ipython\n",
    "from fbprophet import Prophet\n",
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coinmarketcap.com/api/\n",
    "exchange_id = ex_cfg.POLONIEX\n",
    "asset = Asset(coins.ETH, coins.BTC)\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "end = datetime.datetime(year=2018, month=1, day=1)\n",
    "timeframe = Timeframe.ONE_DAY\n",
    "exchange = load_exchange(exchange_id)\n",
    "df = get_price_data(asset, exchange_id, timeframe, start, end)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_period = 24\n",
    "prior_periods = 12\n",
    "\n",
    "close_col = ohlcv_feed.get_col_name('close', asset.symbol, exchange_id)\n",
    "volume_col = ohlcv_feed.get_col_name('volume', asset.symbol, exchange_id)\n",
    "columns = [close_col]\n",
    "\n",
    "close_utc = df[[close_col, 'utc']]\n",
    "plot_price(close_utc, 'close', asset, exchange_id)\n",
    "\n",
    "split_date =  datetime.datetime(year=2017, month=9, day=1)\n",
    "train = close_utc[close_utc['utc'] < split_date]\n",
    "val = close_utc[close_utc['utc'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_utc.columns = ['y','ds']\n",
    "close_utc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(changepoint_prior_scale=0.01)\n",
    "m.fit(close_utc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=60, freq='1800s')\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "71px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
